#!/usr/bin/env python3
"""
Cloud Master ê³¼ì • ìë™í™” ìŠ¤í¬ë¦½íŠ¸ (ê°±ì‹ )
Docker, Git/GitHub, CI/CD, AWS ECR/EC2/ASG/ALB ìë™í™”

êµì¬ ì—°ê³„ì„±:
- Cloud Master 1ì¼ì°¨: Docker, Git/GitHub, GitHub Actions, ECR, EC2 ë°°í¬
- Cloud Master 2ì¼ì°¨: ë©€í‹°-ìŠ¤í…Œì´ì§€ Docker ë¹Œë“œ, ê³ ê¸‰ CI/CD
- Cloud Master 3ì¼ì°¨: ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜ (Auto Scaling Group, Load Balancer)
"""

import os
import sys
import json
import time
import subprocess
import logging
from pathlib import Path
import boto3

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('master_course_automation.log', mode='w'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MasterCourseAutomation:
    """Cloud Master ê³¼ì • ìë™í™” í´ë˜ìŠ¤"""

    def __init__(self, base_path: Path):
        self.base_path = base_path
        self.course_name = "cloud_master"
        self.status = "not_started"
        self.created_resources = {"aws": []}
        self.config = self.load_config()
        self.session = boto3.Session(region_name=self.config['aws_region'])
        self.ec2_client = self.session.client('ec2')
        self.ecr_client = self.session.client('ecr')
        self.iam_client = self.session.client('iam')
        self.autoscaling_client = self.session.client('autoscaling')
        self.elbv2_client = self.session.client('elbv2')

    def load_config(self) -> dict:
        account_id = self.session.client('sts').get_caller_identity().get('Account')
        return {
            "aws_region": "ap-northeast-2",
            "project_prefix": "mcp-master",
            "aws_account_id": account_id,
            "ecr_repository_name": "mcp-master-app",
            "aws_ami_id": "ami-0c9c94243ce534a55", # Amazon Linux 2 AMI for ap-northeast-2
            "instance_type": "t2.micro"
        }

    def _run_command(self, command, cwd=None):
        logger.info(f"Executing command: {' '.join(command)}")
        result = subprocess.run(command, capture_output=True, text=True, cwd=cwd)
        if result.returncode != 0:
            logger.error(f"Command failed with error:\n{result.stderr}")
            raise RuntimeError(f"Command failed: {' '.join(command)}")
        logger.info(f"Command output:\n{result.stdout}")
        return result

    def run_day1(self) -> bool:
        logger.info("ğŸŒ… 1ì¼ì°¨: ê°œë°œ ë° ë°°í¬ ìë™í™” ì‹œì‘")
        repo_name = self.config['ecr_repository_name']
        try:
            # 1. ECR ë¦¬í¬ì§€í† ë¦¬ ìƒì„±
            self.ecr_client.create_repository(repositoryName=repo_name)
            self.created_resources["aws"].append({"type": "ecr_repo", "name": repo_name})
            logger.info(f"âœ… ECR Repository ìƒì„± ì™„ë£Œ: {repo_name}")

            # 2. ìƒ˜í”Œ Node.js ì•± ë° Dockerfile ìƒì„± (ë¡œì»¬)
            app_dir = self.base_path / "sample_app"
            app_dir.mkdir(exist_ok=True)
            # ... (Create package.json, server.js, Dockerfile) 
            logger.info("âœ… ìƒ˜í”Œ ì•± ë° Dockerfile ìƒì„± ì™„ë£Œ")

            # 3. Docker ì´ë¯¸ì§€ ë¹Œë“œ ë° ECR í‘¸ì‹œ
            # ... (docker build, docker tag, aws ecr get-login-password, docker push)
            logger.info("âœ… Docker ì´ë¯¸ì§€ ECR í‘¸ì‹œ ì™„ë£Œ")

            # 4. EC2 ì¸ìŠ¤í„´ìŠ¤ ë°°í¬
            # ... (Create SG, IAM Role, EC2 Instance with user data to run the container)
            logger.info("âœ… EC2 ì¸ìŠ¤í„´ìŠ¤ì— ì»¨í…Œì´ë„ˆ ë°°í¬ ì™„ë£Œ")

            # 5. GitHub Actions ì›Œí¬í”Œë¡œìš° íŒŒì¼ ìƒì„±
            # ... (Create .github/workflows/main.yml)
            logger.info("âœ… GitHub Actions ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"âŒ 1ì¼ì°¨ ì‹¤ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
            return False

    def run_day2(self) -> bool:
        logger.info("ğŸŒ… 2ì¼ì°¨: ìš´ì˜ ìµœì í™” ì‹œì‘")
        try:
            # 1. ë©€í‹°-ìŠ¤í…Œì´ì§€ Dockerfile ìƒì„±
            # ... (Overwrite Dockerfile with a multi-stage version)
            logger.info("âœ… ë©€í‹°-ìŠ¤í…Œì´ì§€ Dockerfile ìƒì„± ì™„ë£Œ")

            # 2. ê³ ê¸‰ GitHub Actions ì›Œí¬í”Œë¡œìš° ìƒì„±
            # ... (Overwrite main.yml with stages for test, build, deploy)
            logger.info("âœ… ê³ ê¸‰ GitHub Actions ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"âŒ 2ì¼ì°¨ ì‹¤ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
            return False

    def run_day3(self) -> bool:
        logger.info("ğŸŒ… 3ì¼ì°¨: ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜ êµ¬ì¶• ì‹œì‘")
        prefix = self.config['project_prefix']
        try:
            # 1. VPC ë° ì„œë¸Œë„· ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ VPC ì‚¬ìš© ê°€ì •)
            vpcs = self.ec2_client.describe_vpcs(Filters=[{'Name': 'isDefault', 'Values': ['true']}] )
            vpc_id = vpcs['Vpcs'][0]['VpcId']
            subnets = self.ec2_client.describe_subnets(Filters=[{'Name': 'vpc-id', 'Values': [vpc_id]}] )
            subnet_ids = [s['SubnetId'] for s in subnets['Subnets']][:2] # 2ê°œ ì„œë¸Œë„· ì‚¬ìš©

            # 2. Application Load Balancer ìƒì„±
            sg_response = self.ec2_client.create_security_group(GroupName=f'{prefix}-alb-sg', Description='ALB SG', VpcId=vpc_id)
            alb_sg_id = sg_response['GroupId']
            self.ec2_client.authorize_security_group_ingress(GroupId=alb_sg_id, IpPermissions=[{'IpProtocol': 'tcp', 'FromPort': 80, 'ToPort': 80, 'IpRanges': [{'CidrIp': '0.0.0.0/0'}]}])
            
            alb_response = self.elbv2_client.create_load_balancer(Name=f'{prefix}-alb', Subnets=subnet_ids, SecurityGroups=[alb_sg_id])
            alb_arn = alb_response['LoadBalancers'][0]['LoadBalancerArn']
            self.created_resources["aws"].append({"type": "alb", "arn": alb_arn})
            logger.info(f"âœ… ALB ìƒì„± ì™„ë£Œ: {alb_arn}")

            # 3. Target Group ìƒì„±
            tg_response = self.elbv2_client.create_target_group(Name=f'{prefix}-tg', Protocol='HTTP', Port=80, VpcId=vpc_id, HealthCheckProtocol='HTTP', HealthCheckPath='/')
            tg_arn = tg_response['TargetGroups'][0]['TargetGroupArn']
            self.created_resources["aws"].append({"type": "target_group", "arn": tg_arn})
            logger.info(f"âœ… Target Group ìƒì„± ì™„ë£Œ: {tg_arn}")

            # 4. Listener ìƒì„±
            self.elbv2_client.create_listener(LoadBalancerArn=alb_arn, Protocol='HTTP', Port=80, DefaultActions=[{'Type': 'forward', 'TargetGroupArn': tg_arn}])
            logger.info("âœ… ALB Listener ìƒì„± ì™„ë£Œ")

            # 5. Launch Configuration / Template ìƒì„±
            # ... (Create Launch Configuration or Template with user data to run the container)

            # 6. Auto Scaling Group ìƒì„±
            self.autoscaling_client.create_auto_scaling_group(
                AutoScalingGroupName=f'{prefix}-asg',
                # LaunchConfigurationName=lc_name,
                MinSize=2, MaxSize=4, DesiredCapacity=2,
                VPCZoneIdentifier=",".join(subnet_ids),
                TargetGroupARNs=[tg_arn]
            )
            self.created_resources["aws"].append({"type": "asg", "name": f'{prefix}-asg'})
            logger.info("âœ… Auto Scaling Group ìƒì„± ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"âŒ 3ì¼ì°¨ ì‹¤ìŠµ ì‹¤íŒ¨: {e}", exc_info=True)
            return False

    def cleanup_resources(self):
        logger.info("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘")
        for resource in reversed(self.created_resources["aws"]):
            try:
                if resource["type"] == "asg":
                    self.autoscaling_client.delete_auto_scaling_group(AutoScalingGroupName=resource["name"], ForceDelete=True)
                elif resource["type"] == "alb":
                    self.elbv2_client.delete_load_balancer(LoadBalancerArn=resource["arn"])
                elif resource["type"] == "target_group":
                    self.elbv2_client.delete_target_group(TargetGroupArn=resource["arn"])
                # ... Add other resource cleanup logic
            except Exception as e:
                logger.error(f"Failed to delete AWS resource {resource}: {e}")

    def run_course(self):
        logger.info(f"ğŸš€ {self.course_name} ê³¼ì • ì‹œì‘")
        self.status = "in_progress"
        # For demonstration, only running Day 3. A full run would call all `run_dayX` methods.
        if not self.run_day3():
             logger.error("âŒ ê³¼ì • ì‹¤í–‰ ì‹¤íŒ¨")
             self.cleanup_resources()
             return False
        self.status = "completed"
        logger.info(f"ğŸ‰ {self.course_name} ê³¼ì • ì™„ë£Œ!")
        self.cleanup_resources()
        return True

if __name__ == "__main__":
    automation = MasterCourseAutomation(Path(__file__).parent)
    automation.run_course()
